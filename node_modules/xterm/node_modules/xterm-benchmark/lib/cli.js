#!/usr/bin/env node
"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
const commander = require("commander");
const index_1 = require("./index");
const appRoot = require("app-root-path");
const fs = require("fs");
const path = require("path");
const helper_1 = require("./helper");
function main() {
    return __awaiter(this, void 0, void 0, function* () {
        commander
            .name('xterm-benchmark')
            .usage('[files]')
            .version(require('../package.json').version, '-v, --version')
            /**
             * --tree <file>
             * Outputs a tree of contexts and perf cases defined
             * in <file>. This is useful to grab a single perf case path
             * and run it single with --single during development.
             */
            .option('-t, --tree <file>', 'show contex and perf case tree')
            /**
             * --single <path>
             * Runs <path> without the siblings. Path denotes a single context
             * or perf case, grab it from --tree.
             * A file always holds a root context, thus calling xterm-benchmark
             * for single file with -s <filepath> is the same as omitting -s.
             */
            .option('-s, --single <path>', 'run single context or perf case')
            /**
             * - repeat <num>
             * Cmdline override to repeat all perf cases <num> times.
             * Repeat setting precedence (least to highest):
             *    - 1 (defined as library default)
             *    - config file
             *    - individual setting at perf case
             *    - cmdline -r switch
             */
            .option('-r, --repeat <num>', 'repeat cases <num> times', parseInt)
            /**
             * TODO (same rules as repeat)
             */
            .option('-t, --timeout <num>', 'set timeout to <num> msec', parseInt)
            /**
             * --output <path>
             * Set the log output to <path>. The log output contains summaries
             * from all run perf cases as newline delimited json.
             * Defaults to <APP_PATH>/<EPOCH>.log (APP_PATH defaults to ./benchmark).
             */
            .option('-o, --output <path>', 'set output path (defaults to <APP_PATH>/<EPOCH>.log)')
            /**
             * --silent
             * Silent stdout. By default the console receives some useful
             * human readable information about running perf cases and results.
             * Note stderr is not affected by this setting.
             */
            .option('-S, --silent', 'no console log output')
            /**
             * --json
             * Silents the console (--silent) and sets output path to stdout.
             * Any other --output setting will be ignored.
             * The output format is newline delimited json.
             * Useful for post processing results by piping.
             */
            .option('-j, --json', 'output json to stdout, equals "-S -l /dev/stdout"')
            /**
             * --full
             * By default only aggregated perf case summaries are reported.
             * With this switch the reports will contain all results of
             * all iterations.
             */
            .option('-f, --full', 'include full results in reports')
            /**
             * --fail
             * For eval runs against a baseline missing results
             * are not handled as errors by default (will exit normally).
             * To ensure the eval set contains the same perf cases as
             * the baseline set this switch.
             */
            .option('-F, --fail', 'also fail on missings')
            /**
             * --baseline
             * Marks the current run as a baseline. Any later eval
             * run (--eval) will be tested against this baseline.
             * Note that any report from a previous run can act
             * as a baseline (see --against), this switch is merely
             * a convenient shortcut and will store the report data
             * under <APP_PATH>/baseline.log. Later calls with
             * --baseline will override this baseline.
             */
            .option('-b, --baseline', 'mark run as baseline data')
            /**
             * --config
             * Provide a config file for xterm-benchmark. Not
             * mandatory, still a must have for advanced setups to
             * fine tune the perf case and eval settings.
             */
            .option('-c, --config <path>', 'path to config file')
            /**
             * --eval
             * Evaluates perf case summary results reported by the current
             * run against a baseline. The baseline defaults to the last
             * run marked with --baseline, provide a custom baseline
             * with --against.
             * The automated evaluation happens on all summary results, that
             * contain statistical values. Those include by default a mean,
             * median, standard deviation and the coefficient of variation.
             * With a skiplist in the config file tests for unwanted values
             * can be skipped.
             * The automated evaluation compares the values against a given tolerance
             * and suceeds if the tested one is within the tolerance of the baseline.
             * Adjust the tolerance in the config file, the default range of
             * 0.25 - 4 is rather silly for most cases.
             * Reports the eval results afterwards and exits either
             * with 0 (success) or 2 (failure).
             */
            .option('-e, --eval', 'eval run against baseline data')
            /**
             * --against <path>
             * Give a report file under <path> as baseline and eval the current
             * run against this baseline.
             */
            .option('-a, --against <path>', 'baseline data path to eval against')
            .parse(process.argv);
        if (commander.tree) {
            index_1.showTree(commander.tree);
            return;
        }
        if (commander.json) {
            commander.silent = true;
            commander.output = '/dev/stdout';
        }
        let APP_PATH = path.join(appRoot.path, 'benchmark');
        if (commander.config) {
            try {
                const config = require(path.resolve(commander.config));
                if (config.APP_PATH) {
                    APP_PATH = path.resolve(config.APP_PATH);
                }
                if (config.evalConfig) {
                    index_1.EVAL_CONFIG.tolerance = Object.assign(index_1.EVAL_CONFIG.tolerance, config.evalConfig.tolerance);
                    index_1.EVAL_CONFIG.skip = config.evalConfig.skip || index_1.EVAL_CONFIG.skip;
                }
                if (config.defaultOptions) {
                    helper_1.mapObjectValues(config.defaultOptions, (value, name) => index_1.DEFAULT_OPTIONS[name] = value);
                }
            }
            catch (e) {
                console.error(e);
                process.exit(1);
            }
        }
        if (!fs.existsSync(APP_PATH)) {
            fs.mkdirSync(APP_PATH);
        }
        const EPOCH = (new Date).getTime();
        index_1.LOGPATHS.push(commander.output ? path.resolve(commander.output) : path.join(APP_PATH, `${EPOCH}.log`));
        let baselinePath = '';
        if (commander.baseline) {
            baselinePath = path.join(APP_PATH, `baseline.log`);
            index_1.LOGPATHS.push(baselinePath);
        }
        let evalPath = '';
        if (commander.eval || commander.against) {
            if (commander.against) {
                baselinePath = path.resolve(commander.against);
            }
            else {
                baselinePath = path.join(APP_PATH, `baseline.log`);
            }
            evalPath = path.join(APP_PATH, `current.log`);
            index_1.LOGPATHS.push(evalPath);
        }
        index_1.LOGPATHS.forEach(path => fs.writeFileSync(path, ''));
        const overrides = {};
        if (commander.repeat) {
            overrides.repeat = commander.repeat;
        }
        if (commander.timeout) {
            overrides.timeout = commander.timeout;
        }
        if (commander.full) {
            overrides.reportFullResults = true;
        }
        Object.assign(index_1.CMDLINE_OVERRIDES, overrides);
        if (commander.silent) {
            console.log = () => { };
        }
        try {
            if (commander.single) {
                const path = commander.single.split('|');
                yield index_1.run(path);
            }
            else {
                const perfFiles = commander.args;
                for (let i = 0; i < perfFiles.length; ++i) {
                    yield index_1.run([perfFiles[i]]);
                }
            }
        }
        catch (error) {
            console.error(error);
            index_1.LOGPATHS.forEach(path => fs.appendFileSync(path, JSON.stringify({ type: 3 /* Error */, data: 'error' }, null) + '\n'));
            process.exit(1);
        }
        if (commander.baseline) {
            try {
                index_1.showBaselineData(baselinePath);
            }
            catch (error) {
                console.error(error);
                index_1.LOGPATHS.forEach(path => fs.appendFileSync(path, JSON.stringify({ type: 3 /* Error */, data: 'error' }, null) + '\n'));
                process.exit(1);
            }
        }
        if (commander.eval || commander.against) {
            try {
                const stats = index_1.evalRun(baselinePath, evalPath);
                if (stats.summary.failed) {
                    process.exit(2);
                }
                if (stats.summary.missing && commander.fail) {
                    process.exit(2);
                }
            }
            catch (error) {
                console.error(error);
                index_1.LOGPATHS.forEach(path => fs.appendFileSync(path, JSON.stringify({ type: 3 /* Error */, data: 'error' }, null) + '\n'));
                process.exit(1);
            }
        }
    });
}
if (require.main === module) {
    main();
}
//# sourceMappingURL=cli.js.map